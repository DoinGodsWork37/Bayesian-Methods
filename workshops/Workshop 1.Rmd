---
title: "Bayesian Methodologies - Lecture 8 - Workshop 1"
author: "Joshua Goldberg"
date: "12 November 2018"
output: html_document
editor_options: 
  chunk_output_type: inline
---

1 Multiple Regression
1.1 Example 1: Two significant predictors
Prepare data for multiple linear regression with 2 independent significant predictors.

```{r Preamble, echo=FALSE}
# Enter package in p_load()
# If package is not installed, p_load() will install and load the package
if(!"pacman" %in% rownames(installed.packages())) {
  install.packages("pacman")
  }
pacman::p_load(tidyverse, ggthemes, here, rstan)

# Set default ggplot theme to tufte
theme_set(ggthemes::theme_tufte())
```

# 1 Multiple Regression
## Generate data
```{r}
set.seed(3526)
Ntotal <- 500
x <- cbind(rnorm(Ntotal, mean = 20, sd = 4), 
           rnorm(Ntotal, mean = 10, sd = 6)
           )
Nx <- ncol(x)
y <- 4 + 1.1*x[,1] + 3*x[,2] + rnorm(Ntotal, mean = 0, sd = 1)
dataListRegression<-list(Ntotal=Ntotal,
                  y=y,
                  x=as.matrix(x),
                  Nx=Nx)
```

The diagram of the model shows hierarchical structure with normal priors for intercept beta0 and slopes betai, i=1,2.

Description of the model corresponding to the diagram:
```{r}
modelString<-"
data {
    int<lower=1> Ntotal;
    int<lower=1> Nx;
    vector[Ntotal] y;
    matrix[Ntotal, Nx] x;
}
transformed data {
    real meanY;
    real sdY;
    vector[Ntotal] zy; // normalized
    vector[Nx] meanX;
    vector[Nx] sdX;
    matrix[Ntotal, Nx] zx; // normalized
    
    meanY = mean(y);
    sdY = sd(y);
    zy = (y - meanY) / sdY;
    for ( j in 1:Nx ) {
        meanX[j] = mean(x[,j]);
        sdX[j] = sd(x[,j]);
        for ( i in 1:Ntotal ) {
            zx[i,j] = ( x[i,j] - meanX[j] ) / sdX[j];
        }
    }
}
parameters {
    real zbeta0;
    vector[Nx] zbeta;
    real<lower=0> nu;
    real<lower=0> zsigma;
}
transformed parameters{
    vector[Ntotal] zy_hat;
    zy_hat = zbeta0 + zx * zbeta;
}
model {
    zbeta0 ~ normal(0, 2);
    zbeta  ~ normal(0, 2);
    nu ~ exponential(1/30.0);
    zsigma ~ uniform(1.0E-5 , 1.0E+1);
    zy ~ student_t(1+nu, zy_hat, zsigma); // 1 + nu exponential starts with zero, nu starts with 1, so we shift
}
generated quantities { 
    // Transform to original scale:
    real beta0; 
    vector[Nx] beta;
    real sigma;
    // .* and ./ are element-wise product and divide
    beta0 = zbeta0*sdY  + meanY - sdY * sum( zbeta .* meanX ./ sdX );
    beta = sdY * ( zbeta ./ sdX );
    sigma = zsigma * sdY;
}"

RobustMultipleRegressionDso <- stan_model(model_code = modelString)
```

Fit the model:
```{r}
fit1 <- sampling(
  RobustMultipleRegressionDso,
  data = dataListRegression,
  pars = c("beta0", "beta", "nu", "sigma"),
  iter = 5000,
  chains = 2,
  cores = 2
)
```

Nothing to worry about with auto-correlations.
```{r}
stan_ac(fit1)
```

```{r}
stan_trace(fit1)
```

Look at the results

* Strong correlation between beta0 and beta1, and beta 0 and beta2

* $\nu$ is very large; looks like a Gaussian distribution; around 20 Gaussian starts; this distribution is skewed upgrade so closer to Gaussian

* $\nu$ is the responsible for the tail of t-distribution; when it gets large we do not have fat tails (Gaussian); if $\nu$ is small (below 10), then we have t-distribution (fat tail)

* $\nu$ t-distrribution can be parameterized with $\nu$; responsible for degrees of freedom

* $\sigma$ accuracy estimate and close to what we simulated

* $\beta1$ and $\beta2$ are reasonably narrow

* Parameter estimates are close to what we expect

```{r}
summary(fit1)$summary[, c(1, 3, 4, 8, 9)]
pairs(fit1, pars = c("beta0", "beta[1]", "beta[2]"))
plot(fit1, pars = "nu")
plot(fit1, pars = "sigma")
plot(fit1, pars = "beta0")
plot(fit1, pars = "beta[1]")
plot(fit1, pars = "beta[2]")
```

Analyze fitted model using shinystan:
```{r, eval=FALSE}
library(shinystan)
launch_shinystan(fit1)
```

Conclusions:
1. Normality parameter ?? is large enough to consider normal distribution: 2.5% HDI level is 12.5837006, mean value is 49.7796287. Not surprising: we simulated normal model.
2. Parameters beta0 and beta1 are significantly negatively correlated, as expected.
3. Parameters beta0 and beta2 are also negatively correlated, but correlation is not so strong.
4. All parameter estimates are close to what we simulated.

# 1.2 Example 2: Insignificant predictor
Read the data from homework assignment of week 7 of Statistical Analysis (31007).

```{r}
Regression.Data <- read_csv("documents-MScA Statistical Analysis 31007-MScA 31007 Lecture 7-DataForRegressionANOVA.csv") %>% 
  as.matrix()

head(Regression.Data)
```

Prepare the data for Stan.
```{r}
Ntotal <- nrow(Regression.Data)
x <- Regression.Data[,2:3]
head(x)
```

```{r}
Nx <- ncol(x)
y <- Regression.Data[, 1]
dataListInsig <- list(
  Ntotal = Ntotal,
  y = y,
  x = as.matrix(x),
  Nx = Nx
)
```

Run MCMC using the same DSO.
```{r}
fit2 <- sampling(
  RobustMultipleRegressionDso,
  data = dataListInsig,
  pars = c("beta0", "beta", "nu", "sigma"),
  iter = 5000,
  chains = 2,
  cores = 2
)
```

Analyze the results.
```{r}
summary(fit2)$summary[, c(1, 3, 4, 8, 9)]
```

```{r}
pairs(fit2, pars = c("beta0", "beta[1]", "beta[2]"))
```

```{r}
plot(fit2, pars = "nu")
plot(fit2, pars = "sigma")
plot(fit2, pars = "beta0")
plot(fit2, pars = "beta[1]")
plot(fit2, pars = "beta[2]")
```

We see that parameter beta2 is not significant.
However, there is no strong correlation or redundancy between the predictors.

Compare with the output of linear model:
```{r}
pairs(Regression.Data)
```

```{r}
summary(lm(Output ~ ., data = as.data.frame(Regression.Data)))
```

## 1.3 Correlated predictors
### 1.3.1 Strong correlation

Create a data set with strongly correlated predictors.
```{r}
set.seed(83945)
Ntotal <- 500
x1 <- rnorm(Ntotal, mean = 20, sd = 4)
x2<-1-1.5*x1+rnorm(Ntotal, mean=0, sd = .1)
x<-cbind(x1,x2)           
plot(x)
```

```{r}
Nx <- ncol(x)
y <- 4 + .2*x[,1] + 3*x[,2]+rnorm(Ntotal, mean = 0, sd = 1)
plot(x[,1],y)
plot(x[,2],y)
```

```{r}
fitlm <- lm(y ~ x[, 1] + x[, 2])
summary(fitlm)
```

```{r}
drop1(fitlm)
```

```{r}
dataListShrink2 <- list(
  Ntotal = Ntotal,
  y = y,
  x = as.matrix(x),
  Nx = Nx
)
```

Note that actual coefficient for x[,1] is+0.2, but slope on the plot plot(x[,1],y) is negative.
Also note that estimated model coefficients are different from actual because of correlation:
```{r}
cbind(actual = c(4, .2, 3), estimated = fitlm$coefficients)
```

Run the chains and analyze the results.
```{r}
tStart <- proc.time()
fit3 <- sampling(
  RobustMultipleRegressionDso,
  data = dataListShrink2,
  pars = c("beta0", "beta", "nu", "sigma"),
  iter = 5000,
  chains = 2,
  cores = 2
)
tEnd <- proc.time()
tEnd - tStart
```

How long did it take runing MCMC? Why so long? It takes long because of the correlation between the predictors, so MCMC has a narrow pathway to explore; tree depth should be longer if you have strong relationship. 

The points will be informative later on.
```{r}
stan_dens(fit3)
```

* $\nu$ is not small so no fat tails

* We know the correlation is strong and negative between $x1$ and $x2$; in order to counter balance, the betas have to move in the opposite direction; parameters will be strongly correlated but in the opposite direction.

* Similar results to linear model

* $\beta1$ HDI contains zero, so it"s not significant

```{r}
stan_ac(fit3, separate_chains = T)
summary(fit3)$summary[, c(1, 3, 4, 8, 9)]
pairs(fit3, pars = c("beta0", "beta[1]", "beta[2]"))
plot(fit3, pars = "nu")
plot(fit3, pars = "sigma")
plot(fit3, pars = "beta0")
plot(fit3, pars = "beta[1]")
plot(fit3, pars = "beta[2]")
```

General signs of collinear predictors:

High correlation between slopes (compensating sign)
Wide posterior distributions for slopes
Increased autocorrelation for slopes

```{r}
pairs(cbind(y, x1, x2))
cbind(
  actual = c(4, .2, 3),
  estimatedLm = fitlm$coefficients,
  estimatedBayes = summary(fit3)$summary[1:3, 1]
)
```

Linear model shows the same information as Bayesian.

## 1.3.2 Collinearity
In case when predictors have strong collinearity, linear model may stop working.
Simulate the same model as in the previous section, but make predictors collinear.

```{r}
set.seed(83945)
Ntotal <- 500
x1 <- rnorm(Ntotal, mean = 20, sd = 4)
x2 <- 1 - 1.5 * x1 + rnorm(Ntotal, mean = 0, sd = .000001)
x <- cbind(x1, x2)
plot(x)
```

```{r}
Nx <- ncol(x)
y <- 4 + .2 * x[, 1] + 3 * x[, 2] + rnorm(Ntotal, mean = 0, sd = 1)
plot(x[, 1], y)
plot(x[, 2], y)
```

Sums of squares are practically zero, so drop1 does not say to drop anyything.
```{r}
dataListShrink2c <- list(
  Ntotal = Ntotal,
  y = y,
  x = as.matrix(x),
  Nx = Nx
)
(lmFit <- lm(y ~ x1 + x2))
summary(lmFit)
drop1(lmFit)
```

Linear model stops working.

Simulate Markov chains.
```{r}
tStart <- proc.time()
fit3c <- sampling(
  RobustMultipleRegressionDso,
  data = dataListShrink2c,
  pars = c("beta0", "beta", "nu", "sigma"),
  iter = 5000,
  chains = 1,
  cores = 2
)
tEnd <- proc.time()
tEnd - tStart
```

With collinear predictors model definitely takes much longer time to simulate.

Distributions become wider with perfect multi-collinearity (we are less certain about the results; HDI will be wider)
```{r}
stan_dens(fit3c)
stan_ac(fit3c, separate_chains = T)
```

```{r}
summary(fit3c)$summary[, c(1, 3, 4, 8, 9)]
pairs(fit3c, pars = c("beta0", "beta[1]", "beta[2]"))
```

* Zero dots mean there were too much depth of trees; too many zigzags (this is not critical)

* May be solved by increasing tree depth

* No big change in $\nu$; still Gaussian model

* Main thing we noticed is wider HDIs, but it worked

* Three consequences: yellow dots, long memory, and wide distributions

```{r}
plot(fit3c, pars = "nu")
plot(fit3c, pars = "sigma")
plot(fit3c, pars = "beta0")
plot(fit3c, pars = "beta[1]")
plot(fit3c, pars = "beta[2]")
```

Markov chains may go over limit on tree depths (yellow dots on pairs graph).
But Bayesian method still works. It shows that one of the slopes is not significantly different from zero.

# 2 Shrinkage of regression coefficients
When there are many candidate predictors in the model it may be useful to "motivate" them to become closer to zero if they are not very strong.
One way to do it is to:

Set a prior distribution for slopes as Student instead of normal;
Make mean of that distribution equal to zero;
Make normality parameter ?? small and dispersion parameter sigma also small: like in the following diagram.
Small sigma forces slopes to shrink towards zero mean. At the same time small ?? makes the tails fat enough to allow some strong slopes to be outliers.

Parameter sigma of the prior for regression coefficients betaj can be either fixed, or given its own prior and estimated.

In the former case all coefficients will be forced to have the same regularizator, if it is random and estimated from the same data then there is mutual influence between sigma and regression coefficients: if many of them are close to zero then sigma is going to be smaller, which in turn pushes coefficients even closer to zero.

What does this approach remind you of in other courses?

## 2.1 Two significant predictors
Use the same data dataListRegression as in the section 1.1.

Describe the model.
```{r}
modelString <- "
data {
    int<lower=1> Ntotal;
    int<lower=1> Nx;
    vector[Ntotal] y;
    matrix[Ntotal, Nx] x;
}
transformed data {
    real meanY;
    real sdY;
    vector[Ntotal] zy; // normalized
    vector[Nx] meanX;
    vector[Nx] sdX;
    matrix[Ntotal, Nx] zx; // normalized
    
    meanY = mean(y);
    sdY = sd(y);
    zy = (y - meanY) / sdY;
    for ( j in 1:Nx ) {
        meanX[j] = mean(x[,j]);
        sdX[j] = sd(x[,j]);
        for ( i in 1:Ntotal ) {
            zx[i,j] = ( x[i,j] - meanX[j] ) / sdX[j];
        }
    }
}
parameters {
    real zbeta0;
    real<lower=0> sigmaBeta;
    vector[Nx] zbeta;
    real<lower=0> nu;
    real<lower=0> zsigma;
}
transformed parameters{
    vector[Ntotal] zy_hat;
    zy_hat = zbeta0 + zx * zbeta;
}
model {
    zbeta0 ~ normal(0, 2);
    sigmaBeta ~ gamma(2.3,1.3); // mode 0, sd 0.5 
    // we want to allow sigma to be as small as necessary; we want it to be zero, 
    // but let it deviate; when sigma close to zero, sigma is much stronger
    zbeta  ~ student_t(1.0/30.0, 0, sigmaBeta);
    nu ~ exponential(1/30.0);
    zsigma ~ uniform(1.0E-5 , 1.0E+1);
    zy ~ student_t(1+nu, zy_hat, zsigma);
}
generated quantities { 
    // Transform to original scale:
    real beta0; 
    vector[Nx] beta;
    real sigma;
    // .* and ./ are element-wise product and divide
    beta0 = zbeta0*sdY  + meanY - sdY * sum( zbeta .* meanX ./ sdX );
    beta = sdY * ( zbeta ./ sdX );
    sigma = zsigma * sdY;
} "
```

Gamma distribution prior for sigmaBeta is selected to have relatively low mode 1.
```{r}
xGamma <- seq(from = .00001, to = 10, by = .001)
plot(xGamma, dgamma(xGamma, shape = 2.3, rate = 1.3), type = "l")

xGamma[which.max(dgamma(xGamma, shape = 2.3, rate = 1.3))]

RegressionShrinkDso <- stan_model(model_code = modelString)
```

Generate Markov chains in case of 2 significant predictors.
```{r}
tStart <- proc.time()
fit4 <- sampling (
  RegressionShrinkDso,
  data = dataListRegression,
  pars = c("beta0", "beta", "nu", "sigma", "sigmaBeta"),
  iter = 5000,
  chains = 2,
  cores = 2
)

tEnd <- proc.time()
tEnd - tStart
```

Analyze fitted model using shinystan (not shown).
```{r}
stan_dens(fit4)
stan_ac(fit4, separate_chains = T)
summary(fit4)$summary[,c(1,3,4,8,9)]
pairs(fit4,pars=c("beta0","beta[1]","beta[2]"))
plot(fit4,pars="nu")
plot(fit4,pars="sigma")
plot(fit4,pars="beta0")
plot(fit4,pars="beta[1]")
plot(fit4,pars="beta[2]")
```

2.1.1 Analysis and comparison
Compare posterior mean values and 95%-HDI with fit1 (same model, but with no shrinkage).

```{r}
cbind(summary(fit1)$summary[1:3, c(1, 4, 8)], summary(fit4)$summary[1:3, c(1, 4, 8)])
```

Mean values of both fits seem very similar.
Check widths of the HDI for coefficients.

```{r}
cbind(
  summary(fit1)$summary[1:3, c(8)] - summary(fit1)$summary[1:3, c(4)],
  summary(fit4)$summary[1:3, c(8)] - summary(fit4)$summary[1:3, c(4)]
)
```

Shrinkage can be noticed after third digit of all coefficients.
In this example both slopes are significant and they practically did not shrink.

For comparison fit linear model, ridge and lasso regressions to the same data.

Linear model.
```{r}
lmFit <- lm(dataListRegression$y ~ dataListRegression$x[, 1] + dataListRegression$x[, 2])

suppressWarnings(library(glmnet))
# Ridge

set.seed(15)
cv.outRidge = cv.glmnet(x = dataListRegression$x, y = dataListRegression$y, alpha =
                          0)
plot(cv.outRidge)
```

```{r}
(bestlam <- cv.outRidge$lambda.min)

ridgeFit<-glmnet(x=dataListRegression$x,y=dataListRegression$y,
                 alpha=0,lambda=bestlam,standardize = F)
ridge.coef<-predict(ridgeFit,type="coefficients",s=bestlam)

#Lasso.
set.seed(15)
cv.outLasso = cv.glmnet(x = dataListRegression$x, y = dataListRegression$y, alpha = 1)
plot(cv.outLasso)
```


```{r}
(bestlam <- cv.outLasso$lambda.min)

## [1] 0.09718884

lassoFit <- glmnet(
  x = dataListRegression$x,
  y = dataListRegression$y,
  alpha = 1,
  lambda = bestlam,
  standardize = F
)

lasso.coef <- predict(lassoFit, type = "coefficients", s = bestlam)
```

Compare coefficients from all 3 models.

* Lasso and Ridge over-estimated the intercept, but results are the same

* Lasso and Ridge we cannot get confidence intervals.

```{r}
comparison <- cbind(
  summary(fit1)$summary[1:3, c(1, 4, 8)],
  summary(fit4)$summary[1:3, c(1, 4, 8)],
  Ridge = ridge.coef,
  Lasso = lasso.coef,
  Linear = lmFit$coefficients
)
colnames(comparison) <-
  c(
    paste("NoShrinkage", c("mean", "2.5%", "97.5%"), sep = "_"),
    paste("Shrinkage", c("mean", "2.5%", "97.5%"), sep =
            "_"),
    "Ridge",
    "Lasso",
    "Linear"
  )
t(comparison)
## 9 x 3 sparse Matrix of class "dgCMatrix"
##                      beta0  beta[1]  beta[2]
## NoShrinkage_mean  4.087040 1.094885 2.998761
## NoShrinkage_2.5%  3.627614 1.073197 2.983739
## NoShrinkage_97.5% 4.542915 1.116478 3.013578
## Shrinkage_mean    4.093912 1.094526 2.998725
## Shrinkage_2.5%    3.647477 1.072684 2.984639
## Shrinkage_97.5%   4.539803 1.115477 3.013344
## Ridge             4.311223 1.087903 2.990438
## Lasso             4.230668 1.088760 2.996746
## Linear            4.095403 1.094193 2.999459

```

All models show practically no shrinkage relative to linear model.
Both Ridge and Lasso regression have too high estimates of intercept.

## 2.2 Insignificant predictor
Shrink estimates from data `dataListInsig`.
```{r}
tStart <- proc.time()
# fit model
fit5 <- sampling (
  RegressionShrinkDso,
  data = dataListInsig,
  pars = c("beta0", "beta", "nu", "sigma", "sigmaBeta"),
  iter = 5000,
  chains = 2,
  cores = 2
)
tEnd <- proc.time()
tEnd - tStart
##    user  system elapsed
##    0.17    0.11    9.56
```

```{r}
stan_dens(fit5)
stan_ac(fit5, separate_chains = T)
summary(fit5)$summary[, c(1, 3, 4, 8, 9)]
pairs(fit5, pars = c("beta0", "beta[1]", "beta[2]"))
plot(fit5, pars = "nu")
plot(fit5, pars = "sigma")
plot(fit5, pars = "beta0")
plot(fit5, pars = "beta[1]")
plot(fit5, pars = "beta[2]")
```

This time posterior density of `beta[2]` is concentrated at zero.

### 2.2.1 Analysis and comparison
Compare mean levels and HDI widths for fits with and without shrinkage.

```{r}
cbind(summary(fit2)$summary[1:3, c(1, 4, 8)], summary(fit5)$summary[1:3, c(1, 4, 8)])
```

```{r}
cbind(
  summary(fit2)$summary[1:3, c(8)] - summary(fit2)$summary[1:3, c(4)],
  summary(fit5)$summary[1:3, c(8)] - summary(fit5)$summary[1:3, c(4)]
)
```

Parameters shrunk a little more this time, second coefficient shrunk to zero.

Again, fit linear model, ridge and lasso regressions to the same data.

Linear model.
```{r}
lmFit<-lm(dataListInsig$y~dataListInsig$x[,1]+dataListInsig$x[,2])
```

Ridge.
```{r}
set.seed(15)
cv.outRidge = cv.glmnet(x = dataListInsig$x, y = dataListInsig$y, alpha = 0)
plot(cv.outRidge)
```

```{r}
(bestlam <- cv.outRidge$lambda.min)
## [1] 0.08542085

ridgeFit <- glmnet(
  x = dataListInsig$x,
  y = dataListInsig$y,
  alpha = 0,
  lambda = bestlam,
  standardize = F
)

ridge.coef <- predict(ridgeFit, type = "coefficients", s = bestlam)

```

Lasso.

```{r}
set.seed(15)
cv.outLasso = cv.glmnet(x = dataListInsig$x, y = dataListInsig$y, alpha = 1)
plot(cv.outLasso)
```

```{r}
(bestlam <- cv.outLasso$lambda.min)

lassoFit <- glmnet(
  x = dataListInsig$x,
  y = dataListInsig$y,
  alpha = 1,
  lambda = bestlam,
  standardize = F
)

lasso.coef <- predict(lassoFit, type = "coefficients", s = bestlam)

```

Compare coefficients from all 3 models.

Lasso eliminated `Input2`
```{r}
comparison <- cbind(
  summary(fit2)$summary[1:3, c(1)],
  summary(fit5)$summary[1:3, c(1)],
  Ridge = ridge.coef,
  Lasso = lasso.coef,
  Linear = lmFit$coefficients
)

colnames(comparison) <- c("NoShrinkage", "Shrinkage", "Ridge", "Lasso", "Linear")
t(comparison)
```

All models correctly exclude second coefficient.
Ridge shrunk both slopes more than other model.
There is again tendency for Ridge and Lasso to overestimate intercept.

## 2.3 Correlated predictors
Shrink coefficients estimated from `dataListShrink2`.

Lasso is supposed to perform very strongly when predictors are strongly correlated.
```{r}
tStart <- proc.time()

fit6 <- sampling (
  RegressionShrinkDso,
  data = dataListShrink2,
  pars = c("beta0", "beta", "nu", "sigma", "sigmaBeta"),
  iter = 5000,
  chains = 2,
  cores = 2
)
tEnd <- proc.time()
tEnd - tStart
```

Check densities, pairs and individual plots of parameters.
```{r}
stan_dens(fit6)
stan_ac(fit6, separate_chains = T)
summary(fit6)$summary[, c(1, 3, 4, 8, 9)]
pairs(fit6, pars = c("beta0", "beta[1]", "beta[2]"))
plot(fit6, pars = "nu")
plot(fit6, pars = "sigma")
plot(fit6, pars = "beta0")
plot(fit6, pars = "beta[1]")
plot(fit6, pars = "beta[2]")
```

2.3.1 Analysis and comparison
Show mean values and HDI.

```{r}
cbind(summary(fit3)$summary[1:3, c(1, 4, 8)], summary(fit6)$summary[1:3, c(1, 4, 8)])
```


```{r}
cbind(
  summary(fit3)$summary[1:3, c(8)] - summary(fit3)$summary[1:3, c(4)],
  summary(fit6)$summary[1:3, c(8)] - summary(fit6)$summary[1:3, c(4)]
)
```

In this example beta1 shrunk more significantly and is not different from zero.
At the same time beta2 has become more different from zero.
Regularization reinforced one of the two correlated predictors while dumping the other,

Again, fit linear model, ridge and lasso regressions to the same data.

Linear model.
```{r}
lmFit <-
  lm(dataListShrink2$y ~ dataListShrink2$x[, 1] + dataListShrink2$x[, 2])
```

Ridge.

```{r}
set.seed(15)
cv.outRidge = cv.glmnet(x = dataListShrink2$x, y = dataListShrink2$y, alpha = 0)
plot(cv.outRidge)
```

```{r}
(bestlam <-cv.outRidge$lambda.min)

ridgeFit <- glmnet(
  x = dataListShrink2$x,
  y = dataListShrink2$y,
  alpha = 0,
  lambda = bestlam,
  standardize = F
)

ridge.coef <- predict(ridgeFit, type = "coefficients", s = bestlam)
```

Lasso.

```{r}
set.seed(15)
cv.outLasso=cv.glmnet(x=dataListShrink2$x,y=dataListShrink2$y,alpha=1)
plot(cv.outLasso)
```

```{r}
(bestlam <- cv.outLasso$lambda.min)

lassoFit <- glmnet(
  x = dataListShrink2$x,
  y = dataListShrink2$y,
  alpha = 1,
  lambda = bestlam,
  standardize = F
)

lasso.coef <- predict(lassoFit, type = "coefficients", s = bestlam)
```
Compare coefficients from all 3 models.

```{r}
comparison <- cbind(
  summary(fit3)$summary[1:3, c(1)],
  summary(fit6)$summary[1:3, c(1)],
  Ridge = ridge.coef,
  Lasso = lasso.coef,
  Linear = lmFit$coefficients
)
colnames(comparison) <- c("NoShrinkage", "Shrinkage", "Ridge", "Lasso", "Linear")

t(comparison)
```

All models correctly exclude first slope.
Lasso does it decisively, making slope beta1 exactly equal to zero.
Lasso also estimated intercept and beta2 more accurately than other models: recall that for this data set beta0=4, beta2=3.

# 3 Is school financing necessary?
Analysis of SAT scores, example from [K], section 18.3.

These data are analyzed in the article by Deborah Lynn Guber.
The variables observed are the mean SAT score by state, amount of money spent by student, percent of students who take SAT and other variables.

Read the data from file `Guber1999data.csv` available at [K].

When we had smaller number of students taking SAT, scores are higer.
```{r}
# according to section 18.3 @ Kruschke
myData <- read_csv("documents-MScA Bayesian Methods (32014)-MScA 32014 Lecture 9-Guber1999data.csv")  

head(myData)

# Big debate that higher salary showed lower SAT scores
myData %>% select_if(is.numeric) %>% pairs()
plot(myData$Spend, myData$SATT)
```

```{r}
summary(lm(myData$SATT ~ myData$Spend))$coeff
```


The plots show that mean SAT score is negatively correlated with amount of money states spend per student.
These results were used in hot debates about spending money on education to support argument in favor of reducing public support for schools.

Prepare the data.

Use the 2 predictors from the file, plus add 12 randomly generated nuisance predictors.
```{r}
Ntotal <- nrow(myData)
y <- myData$SATT
x <- cbind(myData$Spend, myData$PrcntTake)
colnames(x) <- c("Spend", "PrcntTake")

dataList2Predict <- list(
  Ntotal = Ntotal,
  y = y,
  x = x,
  Nx = ncol(x)
)
```

Generate 12 spurious predictors:
```{r}
set.seed(47405)
NxRand <- 12
for (xIdx in 1:NxRand) {
  xRand = rnorm(Ntotal)
  x = cbind(x, xRand)
  colnames(x)[ncol(x)] = paste0("xRand", xIdx)
}

dataListExtraPredict <- list(
  Ntotal = Ntotal,
  y = y,
  x = x,
  Nx = ncol(x)
)
```

## 3.1 No-shrinkage
Use the same model as in the example of the first section: RobustMultipleRegressionDso.

First, run the model with 2 predictors.

If you include two predictors together, then spending more has positive effect on SAT score.
```{r}
fit_noshrink2Pred <- sampling(
  RobustMultipleRegressionDso,
  data = dataList2Predict,
  pars = c("beta0", "beta", "nu", "sigma"),
  iter = 5000,
  chains = 2,
  cores = 2
)

summary(fit_noshrink2Pred)$summary[, c(1, 4, 8)]
```

It is clear that the slope of "Spend" is significantly positive and slope of "PrcntTake" is significantly negative.

This shows that the negative correlation between SAT scores and the money spent as seen from their scatterplot is illusory: fewer students from underfunded schools take SAT, but these are only students who apply for colleges; students who potentially would receive low SAT scores do not apply to college and do not take the test.

Run MCMC for the model with additional nuisance predictors.

```{r}
fit_noshrinkExtra <- sampling(
  RobustMultipleRegressionDso,
  data = dataListExtraPredict,
  pars = c("beta0", "beta", "nu", "sigma"),
  iter = 5000,
  chains = 2,
  cores = 2
)
```

Here are the results of MCMC.
```{r}
stan_ac(fit_noshrinkExtra, separate_chains = T)
pairs(fit_noshrinkExtra, pars = c("beta0", "beta[1]", "beta[2]"))
plot(fit_noshrinkExtra, pars = c("beta"))
stan_dens(fit_noshrinkExtra, pars = c("beta0", "beta"))
```

All densities look symmetrical: mean values of posterior distributions can be used as point estimates of betas.

`beta[12]` has significant negative slope.
```{r}
summary(fit_noshrinkExtra)$summary[, c(1, 4, 8)]
colnames(x)
```

Note that the coefficient for variable "Spend" is still positive, but the left side of HDI interval is much closer to zero. The coefficient for "PrcntTake" is still significantly negative.
One of the nuisance predictors happened to be significantly negative: beta[12].

As a result of adding nuisance predictors the accuracy of inference becomes lower.

## 3.2 Shrinkage
Analyze the same data with the model encouraging shrinkage of parameters.

First, fit the model without nuisance parameters.
```{r}
fit_shrink <- sampling (
  RegressionShrinkDso,
  data = dataList2Predict,
  pars = c("beta0", "beta", "nu", "sigma", "sigmaBeta"),
  iter = 5000,
  chains = 2,
  cores = 2
)
```

```{r}
stan_ac(fit_shrink, separate_chains = T)
pairs(fit_shrink, pars = c("beta0", "beta", "nu", "sigma", "sigmaBeta"))
plot(fit_shrink, pars = c("beta"))
stan_dens(fit_shrink, pars = c("beta"))
```

Compare with the fit without nuisance parameters and without shrinkage.
```{r}
cbind(summary(fit_noshrink2Pred)$summary[1:4, c(1, 4, 8)], summary(fit_shrink)$summary[1:4, c(1, 4, 8)])
```

First variable shrunk closer to zero: mean value is smaller and left end of the 95%-HDI is closer to zero.

Now fit the model with additional parameters.
```{r}
fit_shrinkExtra <- sampling (
  RegressionShrinkDso,
  data = dataListExtraPredict,
  pars = c("beta0", "beta", "nu", "sigma", "sigmaBeta"),
  iter = 5000,
  chains = 2,
  cores = 2
)
```

* Vertical cloud shows no relationship betwee betas (concentrated at zero)

* beta 12 is spread out so it won't be eliminated

* The peaks in the distributions represent shrinkage!

* When the model has shrinkage, we run from state to state, everytime new value appears, we send it to zero

* Same thing happened with $\beta1$

* $\beta2$ is percentage taking SAT and is the only significant $\beta$

```{r}
stan_ac(fit_shrinkExtra, separate_chains = T)
pairs(
  fit_shrinkExtra,
  pars = c(
    "beta0",
    "beta[1]",
    "beta[2]",
    "beta[3]",
    "beta[4]",
    "beta[11]",
    "beta[12]"
  )
)
pairs(fit_shrinkExtra, pars = c("nu", "sigma", "sigmaBeta"))
plot(fit_shrinkExtra, pars = c("beta"))
stan_dens(fit_shrinkExtra, pars = c("beta"))

#Note characteristic pinched tips of posterior densities for shrunk variables.

summary(fit_shrinkExtra)$summary[, c(1:4, 8)]
```

Parameter beta[12] has shrunk to zero based on 95%-HDI as a result of regularized model.
This helped removing all nuisance parameters.
But shrinkage also removed parameter beta[1] of variable "Spend".

## 3.3 Linear model
Compare with linear model.

Without nuisance predictors:
```{r}
lmSAT <- lm(y ~ x[, 1] + x[, 2])
summary(lmSAT)
confint(lmSAT)
```

Every parameter we add, it increases the confidence interval (and increase degrees of freedom).

With nuisance predictors:
```{r}
lmSATAll <- lm(y ~ ., data = as.data.frame(cbind(y, x)))
summary(lmSATAll)
confint(lmSATAll)[2:3, 2] - confint(lmSATAll)[2:3, 1]
confint(lmSAT)[2:3, 2] - confint(lmSAT)[2:3, 1]
```

## 3.4 Ridge and lasso
```{r}
set.seed(15)
cv.outRidge = cv.glmnet(x = dataListExtraPredict$x, y = dataListExtraPredict$y, alpha = 0)
plot(cv.outRidge)
```

```{r}
(bestlam <- cv.outRidge$lambda.min)
## [1] 7.211403
ridgeFit <- glmnet(
  x = dataListExtraPredict$x,
  y = dataListExtraPredict$y,
  alpha = 0,
  lambda = bestlam,
  standardize = F
)

ridge.coef <- predict(ridgeFit, type = "coefficients", s = bestlam)

set.seed(15)
cv.outLasso = cv.glmnet(x = dataListExtraPredict$x, y = dataListExtraPredict$y, alpha = 1)
plot(cv.outLasso)
```

* Ridge and Lasso are Bayesian models with accurate selected prior so it makes the solution easier

* Underneath it calculates as prior distribution

* They are able to remove unnecessary variables

* But they do not always do that

```{r}
(bestlam <- cv.outLasso$lambda.min)
## [1] 2.30712
lassoFit <- glmnet(
  x = dataListExtraPredict$x,
  y = dataListExtraPredict$y,
  alpha = 1,
  lambda = bestlam,
  standardize = F
)

lasso.coef <- predict(lassoFit, type = "coefficients", s = bestlam)
comparison <- round(
  cbind(
    summary(lmSATAll)$coefficients[, c(1, 4)],
    summary(fit_noshrinkExtra)$summary[1:15, c(1, 4, 8)],
    summary(fit_shrinkExtra)$summary[1:15, c(1, 4, 8)],
    ridge.coef,
    lasso.coef
  ),
  3
)

comparison <- as.matrix(comparison)

colnames(comparison) <-
  c(
    "LM",
    "LM-Pv",
    "NoShrink",
    "NoShrink-L",
    "NoShrink-H",
    "Shrink",
    "Shrink-L",
    "Shrink-H",
    "Ridge",
    "Lasso"
    )

comparison
```

Note that there is no way to extract from ridge and lasso regressions any measure for comparison with zero, like confidence intervals.

Linear model keeps both Spend and PrcntTake and removes with 5% level all nuisance coefficients except xRand10.
Bayesian model without shrinkage does the same.
Bayesian model with shrinkage shrinks to zero all artificial predictors, but it also removes Spend.
Ridge in general is consistent with linear model, but it is not clear if it shrinks any parameters to zero or not.
Lasso fails to shrink to zero several artificial parameters.
